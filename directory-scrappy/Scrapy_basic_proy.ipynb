{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Python and BeautifulSoup\n",
    "\n",
    "\n",
    "Webpage to scrap: COVID19 AIS page [https://www.temple-ais.org/covid19/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de librerías\n",
    "#!pip3 install \n",
    "# Librerías básicas\n",
    "\n",
    "import requests  # To connect to the page we want to scrape from\n",
    "from bs4 import BeautifulSoup # Provides a collection of functions that simplify scraping\n",
    "import pandas as pd # To manage our data\n",
    "import urllib\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "from datetime import date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# Page encoding\n",
    "page = requests.get('https://www.utpl.edu.ec/directorio/index.php?ban=1&id_dep=7&id_citte=7&bandera=0&nom_dep=COORDINACIÓN%20DE%20CÁTEDRAS%20UNESCO%20-%20UTPL')\n",
    "page.encoding = \"utf-8\"\n",
    "print(page) # Print status. 200 is OK\n",
    "def download(url, user_agent='Mozilla/5.0', num_retries=2):\n",
    "    #Descarga una url. En caso de existir errores errores [500 600], intentar nuevamente.\n",
    "    #Función adaptada desde (Lawson Richard, 2015)\n",
    "    print('Descargando: ', url)\n",
    "    headers = {'User-agent': user_agent}\n",
    "    request = urllib.request.Request(url, headers=headers)\n",
    "    try:\n",
    "        html = urllib.request.urlopen(request).read()\n",
    "    except urllib.error.URLError as e:\n",
    "        print('Error descargando:', e.reason)\n",
    "        html = None\n",
    "        if num_retries > 0:\n",
    "            if hasattr(e, 'code') and 500 <= e.code < 600:\n",
    "                return download(url, user_agent, num_retries-1)\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQA_cnn = []\\nblocksQ = soup.find_all(class_=\"question-q\")  # en esta clase están todas las preguntas, \\nblocksA = soup.find_all(class_=\"question-a\") # en esta clase están todas las respuestas\\nblocksC = soup.find_all(class_=\"question-t\")  # en esta clase están todas las categorías de preguntas\\nlenQA = len(blocksQ)\\nfor i in range(lenQA):\\n    blocksc = blocksC[i].find_all(name=\"span\")\\n    blocksc = [c.text for c in blocksc] # obtener el texto de cada categoría\\n    blocksc = \";\".join(blocksc) # una pregunta puede tener varias categorías por tanto se unifican los valores\\n    #print(blocksQ[i])\\n    QA_cnn.append([date.today().strftime(\"%d/%m/%Y\"), _page, blocksc, blocksQ[i].text, blocksA[i].text])\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "QA_cnn = []\n",
    "blocksQ = soup.find_all(class_=\"question-q\")  # en esta clase están todas las preguntas, \n",
    "blocksA = soup.find_all(class_=\"question-a\") # en esta clase están todas las respuestas\n",
    "blocksC = soup.find_all(class_=\"question-t\")  # en esta clase están todas las categorías de preguntas\n",
    "lenQA = len(blocksQ)\n",
    "for i in range(lenQA):\n",
    "    blocksc = blocksC[i].find_all(name=\"span\")\n",
    "    blocksc = [c.text for c in blocksc] # obtener el texto de cada categoría\n",
    "    blocksc = \";\".join(blocksc) # una pregunta puede tener varias categorías por tanto se unifican los valores\n",
    "    #print(blocksQ[i])\n",
    "    QA_cnn.append([date.today().strftime(\"%d/%m/%Y\"), _page, blocksc, blocksQ[i].text, blocksA[i].text])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando:  <Response [200]>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unknown url type: 'Response [200]'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m _date \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m regex \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mt]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      5\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# find_all devuelve una lista con todas las etiquetas que cumplan la clase que se indica\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m, in \u001b[0;36mdownload\u001b[1;34m(url, user_agent, num_retries)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescargando: \u001b[39m\u001b[38;5;124m'\u001b[39m, url)\n\u001b[0;32m      9\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-agent\u001b[39m\u001b[38;5;124m'\u001b[39m: user_agent}\n\u001b[1;32m---> 10\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m     html \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(request)\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:322\u001b[0m, in \u001b[0;36mRequest.__init__\u001b[1;34m(self, url, data, headers, origin_req_host, unverifiable, method)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    320\u001b[0m              origin_req_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, unverifiable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    321\u001b[0m              method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 322\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_url\u001b[49m \u001b[38;5;241m=\u001b[39m url\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munredirected_hdrs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:348\u001b[0m, in \u001b[0;36mRequest.full_url\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_url \u001b[38;5;241m=\u001b[39m unwrap(url)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_url, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfragment \u001b[38;5;241m=\u001b[39m _splittag(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_url)\n\u001b[1;32m--> 348\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:377\u001b[0m, in \u001b[0;36mRequest._parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype, rest \u001b[38;5;241m=\u001b[39m _splittype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_url)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown url type: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_url)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselector \u001b[38;5;241m=\u001b[39m _splithost(rest)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost:\n",
      "\u001b[1;31mValueError\u001b[0m: unknown url type: 'Response [200]'"
     ]
    }
   ],
   "source": [
    "page_1 = 'https://www.utpl.edu.ec/directorio/index.php?ban=1&id_dep=7&id_citte=7&bandera=0&nom_dep=COORDINACIÓN%20DE%20CÁTEDRAS%20UNESCO%20-%20UTPLp'\n",
    "_date = ''\n",
    "regex = re.compile(r'[\\n\\r\\t]')\n",
    "html = download(page) \n",
    "soup = BeautifulSoup(html,\"html.parser\")\n",
    "# find_all devuelve una lista con todas las etiquetas que cumplan la clase que se indica\n",
    "items = soup.findAll(\"div\", {\"class\": \"search_result_item line bgwhite mb1 borderb2 borderlightgrey\"});\n",
    "print(items)\n",
    "len(items)\n",
    "# print(soup.find_all('div'))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for classes in soup.find_all({ }'div'):\n",
    "    #classes.get('class')\n",
    " #   print(classes.get('class'))\n",
    "# search_result_item line  bgwhite mb1 borderb2 borderlightgrey\n",
    "print(soup.find('div',{'class':'line mb2 linemam'}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To parse the page's text\n",
    "soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "# Print HTMl DOC\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print title\n",
    "print(soup.title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the page body\n",
    "print(soup.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering content: \n",
    "# find all div tags with field-items class.\n",
    "\n",
    "items = soup.findAll(\"div\",{\"class\":\"container container_old\"});\n",
    "len(items) # 73 div tags in the page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(items[1]) # Print first div tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for info in items:  # print headers2\n",
    "    h2 = info.find(\"h2\")\n",
    "    if h2:\n",
    "        print(h2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1619150089973,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
